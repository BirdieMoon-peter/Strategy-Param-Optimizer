# -*- coding: utf-8 -*-
"""
é€šç”¨ç­–ç•¥ä¼˜åŒ–æµ‹è¯•è„šæœ¬
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®æ ‡çš„æ•°æ®ã€ç­–ç•¥è„šæœ¬ã€ä¼˜åŒ–ç›®æ ‡ã€LLMç­‰
æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªCSVæ–‡ä»¶æ‰¹é‡ä¼˜åŒ–

ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•ï¼ˆå•ä¸ªæ•°æ®æ–‡ä»¶ï¼‰
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py

  # å¤šä¸ªæ•°æ®æ–‡ä»¶æ‰¹é‡ä¼˜åŒ–
  python run_optimizer.py --data project_trend/data/AG.csv project_trend/data/BTC.csv project_trend/data/ETH.csv --strategy project_trend/src/Aberration.py

  # ä½¿ç”¨é€šé…ç¬¦åŒ¹é…å¤šä¸ªæ–‡ä»¶
  python run_optimizer.py --data project_trend/data/*.csv --strategy project_trend/src/Aberration.py

  # ä½¿ç”¨LLM
  python run_optimizer.py --data project_trend/data/BTC.csv --strategy project_trend/src/Aberration.py --use-llm

  # æŒ‡å®šä¼˜åŒ–ç›®æ ‡å’Œè¯•éªŒæ¬¡æ•°
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py --objective annual_return --trials 100

  # æŒ‡å®šè¦ä¼˜åŒ–çš„å‚æ•°ï¼ˆé€šè¿‡params.txtæ–‡ä»¶ï¼‰
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py --params-file params.txt

  # å®Œæ•´å‚æ•°ï¼ˆå¤šæ–‡ä»¶ï¼‰
  python run_optimizer.py --data project_trend/data/AG.csv project_trend/data/BTC.csv --strategy project_trend/src/Aberration.py --objective sharpe_ratio --trials 50 --use-llm --llm-model xuanyuan --output ./my_results
"""

import sys
import os
import json
import argparse
import glob
import pandas as pd
from pathlib import Path
from datetime import datetime

# æ·»åŠ  Optimizer åˆ°è·¯å¾„
optimizer_path = str(Path(__file__).parent / "optimizer")
if optimizer_path not in sys.path:
    sys.path.insert(0, optimizer_path)

# å¯¼å…¥ä¼˜åŒ–å™¨æ¨¡å—
import universal_optimizer
import universal_llm_client
UniversalOptimizer = universal_optimizer.UniversalOptimizer
UniversalLLMConfig = universal_llm_client.UniversalLLMConfig


def load_target_params(params_file: str) -> list:
    """
    ä»æ–‡ä»¶åŠ è½½è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨
    
    Args:
        params_file: å‚æ•°æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªå‚æ•°å
        
    Returns:
        å‚æ•°ååˆ—è¡¨
    """
    if not Path(params_file).exists():
        raise FileNotFoundError(f"å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {params_file}")
    
    params = []
    with open(params_file, 'r', encoding='utf-8') as f:
        for line in f:
            # å»é™¤ç©ºç™½å­—ç¬¦å’Œæ³¨é‡Š
            param = line.strip()
            if param and not param.startswith('#'):
                params.append(param)
    
    if not params:
        raise ValueError(f"å‚æ•°æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆå‚æ•°: {params_file}")
    
    return params


def load_space_config(config_file: str) -> dict:
    """
    ä» JSON æ–‡ä»¶åŠ è½½å‚æ•°ç©ºé—´é…ç½®
    
    Args:
        config_file: å‚æ•°ç©ºé—´é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        å‚æ•°ç©ºé—´é…ç½®å­—å…¸
    """
    if not Path(config_file).exists():
        raise FileNotFoundError(f"å‚æ•°ç©ºé—´é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # éªŒè¯é…ç½®æ ¼å¼
    if 'param_space' not in config:
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»åŒ…å« 'param_space' å­—æ®µ")
    
    param_space = config['param_space']
    
    # éªŒè¯æ¯ä¸ªå‚æ•°çš„é…ç½®
    for param_name, param_config in param_space.items():
        if 'min' not in param_config or 'max' not in param_config:
            raise ValueError(f"å‚æ•° '{param_name}' å¿…é¡»æŒ‡å®š 'min' å’Œ 'max'")
        if param_config['min'] >= param_config['max']:
            raise ValueError(f"å‚æ•° '{param_name}' çš„ min å¿…é¡»å°äº max")
    
    return param_space


def prepare_data(data_path: str) -> str:
    """
    å‡†å¤‡æ•°æ®æ–‡ä»¶ï¼šç¡®ä¿æœ‰ datetime åˆ—
    
    Args:
        data_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        å¤„ç†åçš„æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    df = pd.read_csv(data_path)
    
    # æ£€æŸ¥å¹¶é‡å‘½åæ—¥æœŸåˆ—
    if 'datetime' not in df.columns:
        if 'date' in df.columns:
            df.rename(columns={'date': 'datetime'}, inplace=True)
            print(f"[æ•°æ®] å·²å°† 'date' åˆ—é‡å‘½åä¸º 'datetime'")
        elif 'time_key' in df.columns:
            df.rename(columns={'time_key': 'datetime'}, inplace=True)
            print(f"[æ•°æ®] å·²å°† 'time_key' åˆ—é‡å‘½åä¸º 'datetime'")
    
    if 'datetime' not in df.columns:
        raise ValueError("æ•°æ®æ–‡ä»¶å¿…é¡»åŒ…å« 'datetime' æˆ– 'date' åˆ—")
    
    # è½¬æ¢æ—¥æœŸæ ¼å¼
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    data_dir = Path(data_path).parent
    asset_name = Path(data_path).stem
    processed_path = data_dir / f"{asset_name}_processed.csv"
    df.to_csv(processed_path, index=False)
    
    print(f"[æ•°æ®] å¤„ç†å®Œæˆ: {len(df)} æ¡è®°å½•")
    print(f"[æ•°æ®] æ—¶é—´èŒƒå›´: {df['datetime'].min()} è‡³ {df['datetime'].max()}")
    
    return str(processed_path)


def create_llm_config(args) -> UniversalLLMConfig:
    """
    åˆ›å»ºLLMé…ç½®
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        LLMé…ç½®å¯¹è±¡
    """
    return UniversalLLMConfig(
        api_type=args.llm_type,
        base_url=args.llm_url,
        model_name=args.llm_model,
        api_key=args.api_key,
        temperature=0.7,
        max_tokens=4096,
        timeout=args.timeout
    )


def print_results(result: dict, output_dir: Path, asset_name: str = None):
    """
    æ‰“å°å’Œä¿å­˜ä¼˜åŒ–ç»“æœ
    
    Args:
        result: ä¼˜åŒ–ç»“æœå­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        asset_name: èµ„äº§åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºè¦†ç›–ç»“æœä¸­çš„åç§°ï¼‰
    """
    print("\n" + "="*60)
    print("âœ… ä¼˜åŒ–å®Œæˆï¼")
    print("="*60)
    
    # æœ€ä¼˜å‚æ•°
    print("\nã€æœ€ä¼˜å‚æ•°ã€‘")
    best_params = result.get('best_parameters', {})
    for param, value in best_params.items():
        if isinstance(value, float):
            print(f"  {param}: {value:.4f}")
        else:
            print(f"  {param}: {value}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print("\nã€æ€§èƒ½æŒ‡æ ‡ã€‘")
    metrics = result.get('performance_metrics', {})
    print(f"  å¤æ™®æ¯”ç‡: {metrics.get('sharpe_ratio', 0):.4f}")
    print(f"  å¹´åŒ–æ”¶ç›Šç‡: {metrics.get('annual_return', 0):.2f}%")
    print(f"  æœ€å¤§å›æ’¤: {metrics.get('max_drawdown', 0):.2f}%")
    print(f"  æ€»æ”¶ç›Šç‡: {metrics.get('total_return', 0):.2f}%")
    print(f"  äº¤æ˜“æ¬¡æ•°: {metrics.get('trades_count', 0)}")
    print(f"  èƒœç‡: {metrics.get('win_rate', 0):.2f}%")
    
    # é€å¹´è¡¨ç°
    yearly = result.get('yearly_performance', {})
    if yearly:
        print("\nã€é€å¹´è¡¨ç°ã€‘")
        # è¿‡æ»¤æ‰æ”¶ç›Šä¸º0ä¸”å›æ’¤ä¸º0çš„å¹´ä»½ï¼ˆå¯èƒ½æ˜¯æ— äº¤æ˜“å¹´ä»½ï¼‰
        active_years = {y: p for y, p in yearly.items() 
                       if p.get('return', 0) != 0 or p.get('drawdown', 0) != 0}
        inactive_years = [y for y, p in yearly.items() 
                         if p.get('return', 0) == 0 and p.get('drawdown', 0) == 0]
        
        for year, perf in sorted(active_years.items()):
            ret = perf.get('return', 0)
            dd = perf.get('drawdown', 0)
            sr = perf.get('sharpe_ratio', 0)
            print(f"  {year}å¹´: æ”¶ç›Š {ret:+.2f}%, å›æ’¤ {dd:.2f}%, å¤æ™® {sr:.4f}")
        
        if inactive_years:
            print(f"  æ— äº¤æ˜“å¹´ä»½: {', '.join(sorted(inactive_years))}")
    
    # LLMè§£é‡Š
    explanation = result.get('llm_explanation', {})
    if explanation and explanation.get('parameter_explanation'):
        print("\nã€LLM åˆ†æã€‘")
        print(f"  {explanation.get('parameter_explanation', '')}")
        
        if explanation.get('key_insights'):
            print("\nå…³é”®æ´å¯Ÿ:")
            for i, insight in enumerate(explanation['key_insights'], 1):
                print(f"  {i}. {insight}")
    
    # ä¿å­˜æ‘˜è¦
    summary_path = output_dir / "optimization_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("ç­–ç•¥ä¼˜åŒ–ç»“æœæ‘˜è¦\n")
        f.write("="*60 + "\n\n")
        
        f.write(f"ä¼˜åŒ–æ—¶é—´: {result.get('optimization_info', {}).get('optimization_time', '')}\n")
        f.write(f"æ ‡çš„: {result.get('optimization_info', {}).get('asset_name', '')}\n")
        f.write(f"ç­–ç•¥: {result.get('optimization_info', {}).get('strategy_name', '')}\n")
        f.write(f"ä¼˜åŒ–ç›®æ ‡: {result.get('optimization_info', {}).get('optimization_objective', '')}\n\n")
        
        f.write("ã€æœ€ä¼˜å‚æ•°ã€‘\n")
        for param, value in best_params.items():
            f.write(f"  {param}: {value}\n")
        
        f.write("\nã€æ€§èƒ½æŒ‡æ ‡ã€‘\n")
        for key, value in metrics.items():
            f.write(f"  {key}: {value}\n")
    
    print(f"\nç»“æœæ‘˜è¦å·²ä¿å­˜è‡³: {summary_path}")


def main():
    parser = argparse.ArgumentParser(
        description="é€šç”¨ç­–ç•¥ä¼˜åŒ–å™¨ï¼ˆæ”¯æŒå¤šCSVæ–‡ä»¶æ‰¹é‡ä¼˜åŒ–ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•ï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py

  # å¤šä¸ªæ•°æ®æ–‡ä»¶æ‰¹é‡ä¼˜åŒ–
  python run_optimizer.py --data project_trend/data/AG.csv project_trend/data/BTC.csv project_trend/data/ETH.csv --strategy project_trend/src/Aberration.py

  # ä½¿ç”¨é€šé…ç¬¦åŒ¹é…å¤šä¸ªæ–‡ä»¶
  python run_optimizer.py --data "project_trend/data/*.csv" --strategy project_trend/src/Aberration.py

  # ä½¿ç”¨æœ¬åœ° Ollama LLM
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py --use-llm

  # ä½¿ç”¨ OpenAI
  python run_optimizer.py --data project_trend/data/BTC.csv --strategy project_trend/src/Aberration.py --use-llm --llm-type openai --api-key sk-xxx

  # æŒ‡å®šä¼˜åŒ–ç›®æ ‡
  python run_optimizer.py --data project_trend/data/AG.csv --strategy project_trend/src/Aberration.py --objective annual_return

ä¼˜åŒ–ç›®æ ‡é€‰é¡¹:
  sharpe_ratio   - å¤æ™®æ¯”ç‡ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
  annual_return  - å¹´åŒ–æ”¶ç›Šç‡
  total_return   - æ€»æ”¶ç›Šç‡
  max_drawdown   - æœ€å¤§å›æ’¤ï¼ˆæœ€å°åŒ–ï¼‰
  calmar_ratio   - å¡ç›æ¯”ç‡
  sortino_ratio  - ç´¢æè¯ºæ¯”ç‡
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--data", "-d",
        nargs='+',
        required=True,
        help="æ ‡çš„æ•°æ® CSV æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå¤šä¸ªæ–‡ä»¶æˆ–é€šé…ç¬¦ï¼ˆå¿…é¡»åŒ…å« datetime/date, open, high, low, close, volume åˆ—ï¼‰"
    )
    parser.add_argument(
        "--strategy", "-s",
        required=True,
        help="ç­–ç•¥è„šæœ¬æ–‡ä»¶è·¯å¾„ï¼ˆ.pyæ–‡ä»¶ï¼Œå¿…é¡»åŒ…å«ç»§æ‰¿ bt.Strategy çš„ç­–ç•¥ç±»ï¼‰"
    )
    parser.add_argument(
        "--multi-data",
        action="store_true",
        help="å°†å¤šä¸ª --data æ–‡ä»¶ä½œä¸ºåŒä¸€ç­–ç•¥çš„å¤šæ•°æ®æºè¾“å…¥ï¼ˆé¡ºåºå³æ•°æ®æºé¡ºåºï¼‰"
    )
    parser.add_argument(
        "--data-names",
        nargs='+',
        default=None,
        help="å¤šæ•°æ®æºçš„åç§°åˆ—è¡¨ï¼ˆéœ€ä¸ --data æ•°é‡ä¸€è‡´ï¼Œä¾‹å¦‚ QQQ TQQQï¼‰"
    )
    
    # ä¼˜åŒ–å‚æ•°
    parser.add_argument(
        "--objective", "-o",
        default="sharpe_ratio",
        choices=["sharpe_ratio", "annual_return", "total_return", "max_drawdown", "calmar_ratio", "sortino_ratio"],
        help="ä¼˜åŒ–ç›®æ ‡ï¼ˆé»˜è®¤: sharpe_ratioï¼‰"
    )
    parser.add_argument(
        "--trials", "-t",
        type=int,
        default=50,
        help="ä¼˜åŒ–è¯•éªŒæ¬¡æ•°ï¼ˆé»˜è®¤: 50ï¼Œå¯ç”¨åŠ¨æ€è¯•éªŒä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰"
    )
    parser.add_argument(
        "--params-file", "-p",
        default=None,
        help="æŒ‡å®šè¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªå‚æ•°åï¼‰ï¼Œä¸æŒ‡å®šåˆ™ä¼˜åŒ–æ‰€æœ‰å‚æ•°"
    )
    parser.add_argument(
        "--space-config", "-S",
        default=None,
        help="å‚æ•°ç©ºé—´é…ç½®æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œç”¨äºæ‰‹åŠ¨æŒ‡å®šå‚æ•°æœç´¢èŒƒå›´ï¼Œå‚è€ƒ space_config_example.json"
    )
    
    # æ•°æ®é¢‘ç‡å‚æ•°
    parser.add_argument(
        "--data-freq", "-f",
        default=None,
        choices=["daily", "1m", "5m", "15m", "30m", "hourly", "auto"],
        help="æ•°æ®é¢‘ç‡ï¼ˆé»˜è®¤: autoè‡ªåŠ¨æ£€æµ‹ï¼‰ã€‚daily=æ—¥çº¿, 1m=1åˆ†é’Ÿ, 5m=5åˆ†é’Ÿ, 15m=15åˆ†é’Ÿ, 30m=30åˆ†é’Ÿ, hourly=å°æ—¶çº¿"
    )
    
    # v2.0 æ–°å¢ï¼šå¢å¼ºé‡‡æ ·å™¨å‚æ•°
    parser.add_argument(
        "--no-enhanced-sampler",
        action="store_true",
        help="ç¦ç”¨å¢å¼ºé‡‡æ ·å™¨ï¼ˆæ­£æ€åˆ†å¸ƒé‡‡æ ·ï¼‰ï¼Œä½¿ç”¨ä¼ ç»Ÿå‡åŒ€é‡‡æ ·"
    )
    parser.add_argument(
        "--no-dynamic-trials",
        action="store_true",
        help="ç¦ç”¨åŠ¨æ€è¯•éªŒæ¬¡æ•°ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å›ºå®šå€¼"
    )
    parser.add_argument(
        "--no-boundary-search",
        action="store_true",
        help="ç¦ç”¨è¾¹ç•ŒäºŒæ¬¡æœç´¢"
    )
    parser.add_argument(
        "--max-boundary-rounds",
        type=int,
        default=2,
        help="è¾¹ç•ŒäºŒæ¬¡æœç´¢æœ€å¤§è½®æ•°ï¼ˆé»˜è®¤: 2ï¼‰"
    )
    
    # LLMå‚æ•°
    parser.add_argument(
        "--use-llm",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨LLMè¾…åŠ©ä¼˜åŒ–"
    )
    parser.add_argument(
        "--llm-type",
        default="ollama",
        choices=["ollama", "openai", "custom"],
        help="LLMç±»å‹ï¼ˆé»˜è®¤: ollamaï¼‰"
    )
    parser.add_argument(
        "--llm-model",
        default="xuanyuan",
        help="LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤: xuanyuanï¼‰"
    )
    parser.add_argument(
        "--llm-url",
        default="http://localhost:11434",
        help="LLM API URLï¼ˆé»˜è®¤: http://localhost:11434ï¼‰"
    )
    parser.add_argument(
        "--api-key",
        default="",
        help="APIå¯†é’¥ï¼ˆOpenAIéœ€è¦ï¼‰"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=180,
        help="LLMè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤: 180ï¼‰"
    )
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        "--output", "-O",
        default="./optimization_results",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./optimization_resultsï¼‰"
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼ˆå‡å°‘è¾“å‡ºï¼‰"
    )
    
    args = parser.parse_args()
    
    # å±•å¼€é€šé…ç¬¦å¹¶æ”¶é›†æ‰€æœ‰æ•°æ®æ–‡ä»¶
    data_files = []
    for pattern in args.data:
        # å°è¯•é€šé…ç¬¦åŒ¹é…
        matched = glob.glob(pattern)
        if matched:
            # ä¿æŒé€šé…ç¬¦åŒ¹é…çš„å±€éƒ¨é¡ºåº
            data_files.extend(sorted(matched))
        elif Path(pattern).exists():
            # ä¸æ˜¯é€šé…ç¬¦ï¼Œæ˜¯ç›´æ¥çš„æ–‡ä»¶è·¯å¾„
            data_files.append(pattern)
        else:
            print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pattern}")
            return 1
    
    # è¿‡æ»¤é CSV æ–‡ä»¶
    data_files = [f for f in data_files if f.endswith('.csv')]
    
    # æ ¹æ®æ¨¡å¼å¤„ç†å»é‡/æ’åº
    if args.multi_data:
        # ä¿æŒé¡ºåºå»é‡ï¼ˆå¤šæ•°æ®æºé¡ºåºå¾ˆé‡è¦ï¼‰
        seen = set()
        ordered_files = []
        for f in data_files:
            if f not in seen:
                seen.add(f)
                ordered_files.append(f)
        data_files = ordered_files
    else:
        # æ‰¹é‡ä¼˜åŒ–æ—¶å»é‡æ’åº
        data_files = list(set(data_files))
        data_files.sort()  # æ’åºä»¥ä¿è¯é¡ºåºä¸€è‡´
    
    if not data_files:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ CSV æ•°æ®æ–‡ä»¶")
        return 1
    
    # å¤šæ•°æ®æºæ¨¡å¼ä¸‹ï¼Œæ ¡éªŒ data_names
    if args.multi_data and args.data_names:
        if len(args.data_names) != len(data_files):
            print("âŒ é”™è¯¯: --data-names æ•°é‡å¿…é¡»ä¸ --data æ–‡ä»¶æ•°é‡ä¸€è‡´")
            return 1
    
    # éªŒè¯ç­–ç•¥æ–‡ä»¶å­˜åœ¨
    if not Path(args.strategy).exists():
        print(f"âŒ é”™è¯¯: ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {args.strategy}")
        return 1
    
    # åŠ è½½ç›®æ ‡å‚æ•°åˆ—è¡¨ï¼ˆå¦‚æœæŒ‡å®šäº†å‚æ•°æ–‡ä»¶ï¼‰
    target_params = None
    if args.params_file:
        if not Path(args.params_file).exists():
            print(f"âŒ é”™è¯¯: å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {args.params_file}")
            return 1
        try:
            target_params = load_target_params(args.params_file)
        except Exception as e:
            print(f"âŒ é”™è¯¯: è¯»å–å‚æ•°æ–‡ä»¶å¤±è´¥: {e}")
            return 1
    
    # åŠ è½½å‚æ•°ç©ºé—´é…ç½®ï¼ˆå¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼‰
    custom_space = None
    if args.space_config:
        if not Path(args.space_config).exists():
            print(f"âŒ é”™è¯¯: å‚æ•°ç©ºé—´é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.space_config}")
            return 1
        try:
            custom_space = load_space_config(args.space_config)
            print(f"[é…ç½®] å·²åŠ è½½è‡ªå®šä¹‰å‚æ•°ç©ºé—´: {list(custom_space.keys())}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: è¯»å–å‚æ•°ç©ºé—´é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return 1
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    if not args.quiet:
        print("\n" + "="*60)
        print("é€šç”¨ç­–ç•¥ä¼˜åŒ–å™¨")
        print("="*60)
        print(f"æ•°æ®æ–‡ä»¶: {len(data_files)} ä¸ª")
        for i, f in enumerate(data_files, 1):
            print(f"  [{i}] {f}")
        print(f"ç­–ç•¥æ–‡ä»¶: {args.strategy}")
        print(f"ä¼˜åŒ–ç›®æ ‡: {args.objective}")
        print(f"è¯•éªŒæ¬¡æ•°: {args.trials}")
        if target_params:
            print(f"æŒ‡å®šå‚æ•°: {target_params}")
        else:
            print(f"æŒ‡å®šå‚æ•°: å…¨éƒ¨å‚æ•°")
        if custom_space:
            print(f"è‡ªå®šä¹‰ç©ºé—´: {list(custom_space.keys())}")
        else:
            print(f"å‚æ•°ç©ºé—´: è‡ªåŠ¨ç”Ÿæˆï¼ˆæ™ºèƒ½è§„åˆ™ï¼‰")
        print(f"ä½¿ç”¨LLM: {'æ˜¯' if args.use_llm else 'å¦'}")
        if args.use_llm:
            print(f"LLMç±»å‹: {args.llm_type}")
            print(f"LLMæ¨¡å‹: {args.llm_model}")
        print("="*60 + "\n")
    
    try:
        # 1. é…ç½®LLMï¼ˆå¦‚æœéœ€è¦ï¼‰
        llm_config = None
        if args.use_llm:
            if args.llm_type == "openai" and not args.api_key:
                print("âš ï¸  è­¦å‘Š: ä½¿ç”¨OpenAIéœ€è¦æä¾› --api-key")
            
            # è®¾ç½®æ­£ç¡®çš„URL
            if args.llm_type == "openai" and args.llm_url == "http://localhost:11434":
                args.llm_url = "https://api.openai.com/v1"
            
            llm_config = create_llm_config(args)
            
            if not args.quiet:
                print(f"[LLM] é…ç½®: {args.llm_type} / {args.llm_model}")
        
        # 2. åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 3. æ‰¹é‡ä¼˜åŒ–æ¯ä¸ªæ•°æ®æ–‡ä»¶ / å¤šæ•°æ®æºä¼˜åŒ–
        all_results = []
        success_count = 0
        fail_count = 0
        
        if args.multi_data:
            # å¤šæ•°æ®æºæ¨¡å¼ï¼šæ‰€æœ‰æ•°æ®æ–‡ä»¶ä½œä¸ºåŒä¸€ç­–ç•¥çš„å¤šæ•°æ®è¾“å…¥
            if not args.quiet:
                print("\n" + "="*60)
                print(f"ğŸ“ˆ [å¤šæ•°æ®æº] å¼€å§‹ä¼˜åŒ–: {len(data_files)} ä¸ªæ•°æ®æº")
                for i, f in enumerate(data_files, 1):
                    print(f"  [{i}] {f}")
                print("="*60)
            
            try:
                # å‡†å¤‡æ•°æ®ï¼ˆé€ä¸ªå¤„ç†ï¼‰
                processed_paths = []
                for data_file in data_files:
                    processed_paths.append(prepare_data(data_file))
                
                # æ•°æ®æºåç§°
                if args.data_names:
                    data_names = args.data_names
                else:
                    data_names = [Path(p).stem.replace('_processed', '') for p in processed_paths]
                
                asset_label = "+".join(data_names)
                
                # åˆ›å»ºè¾“å‡ºå­ç›®å½•
                asset_output_dir = output_dir / asset_label
                asset_output_dir.mkdir(parents=True, exist_ok=True)
                
                # åˆ›å»ºä¼˜åŒ–å™¨
                if not args.quiet:
                    print("\n[ä¼˜åŒ–å™¨] åˆå§‹åŒ–ä¸­...")
                
                optimizer = UniversalOptimizer(
                    data_path=processed_paths,
                    strategy_path=str(Path(args.strategy).absolute()),
                    objective=args.objective,
                    use_llm=args.use_llm,
                    llm_config=llm_config,
                    output_dir=str(asset_output_dir),
                    verbose=not args.quiet,
                    target_params=target_params,
                    custom_space=custom_space,
                    data_names=data_names,
                    data_frequency=args.data_freq
                )
                
                # æ‰§è¡Œä¼˜åŒ–
                use_enhanced = not args.no_enhanced_sampler
                enable_dynamic = not args.no_dynamic_trials
                enable_boundary = not args.no_boundary_search
                
                if not args.quiet:
                    print(f"\n[ä¼˜åŒ–] å¼€å§‹ä¼˜åŒ–...")
                    print(f"[ä¼˜åŒ–] åŸºç¡€è¯•éªŒæ¬¡æ•°: {args.trials}")
                    if use_enhanced:
                        print(f"[ä¼˜åŒ–] é‡‡æ ·ç­–ç•¥: æ­£æ€åˆ†å¸ƒ + è´å¶æ–¯ä¼˜åŒ–")
                    if enable_dynamic:
                        print(f"[ä¼˜åŒ–] åŠ¨æ€è¯•éªŒ: å¯ç”¨ï¼ˆå°†æ ¹æ®å‚æ•°é‡è‡ªåŠ¨è°ƒæ•´ï¼‰")
                    if enable_boundary:
                        print(f"[ä¼˜åŒ–] è¾¹ç•ŒäºŒæ¬¡æœç´¢: å¯ç”¨ï¼ˆæœ€å¤š{args.max_boundary_rounds}è½®ï¼‰\n")
                
                result = optimizer.optimize(
                    n_trials=args.trials,
                    use_enhanced_sampler=use_enhanced,
                    enable_dynamic_trials=enable_dynamic,
                    auto_expand_boundary=enable_boundary,
                    max_expansion_rounds=args.max_boundary_rounds
                )
                
                # æ‰“å°å’Œä¿å­˜ç»“æœ
                print_results(result, asset_output_dir, asset_label)
                
                # è®°å½•ç»“æœ
                all_results.append({
                    'asset': asset_label,
                    'status': 'success',
                    'result': result
                })
                success_count += 1
                
            except Exception as e:
                print(f"\nâŒ å¤šæ•°æ®æºä¼˜åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                all_results.append({
                    'asset': 'multi_data',
                    'status': 'failed',
                    'error': str(e)
                })
                fail_count += 1
        else:
            for idx, data_file in enumerate(data_files, 1):
                # æå–åŸå§‹èµ„äº§åç§°ï¼ˆå»é™¤ _processed åç¼€ï¼‰
                original_asset_name = Path(data_file).stem.replace('_processed', '')
                
                if not args.quiet:
                    print("\n" + "="*60)
                    print(f"ğŸ“ˆ [{idx}/{len(data_files)}] å¼€å§‹ä¼˜åŒ–: {original_asset_name}")
                    print("="*60)
                
                try:
                    # å‡†å¤‡æ•°æ®
                    data_path = prepare_data(data_file)
                    
                    # åˆ›å»ºè¯¥èµ„äº§çš„è¾“å‡ºå­ç›®å½•
                    asset_output_dir = output_dir / original_asset_name
                    asset_output_dir.mkdir(parents=True, exist_ok=True)
                    
                    # åˆ›å»ºä¼˜åŒ–å™¨
                    if not args.quiet:
                        print("\n[ä¼˜åŒ–å™¨] åˆå§‹åŒ–ä¸­...")
                    
                    optimizer = UniversalOptimizer(
                        data_path=data_path,
                        strategy_path=str(Path(args.strategy).absolute()),
                        objective=args.objective,
                        use_llm=args.use_llm,
                        llm_config=llm_config,
                        output_dir=str(asset_output_dir),
                        verbose=not args.quiet,
                        target_params=target_params,
                        custom_space=custom_space,
                        data_frequency=args.data_freq
                    )
                    
                    # æ‰§è¡Œä¼˜åŒ–ï¼ˆv2.0 æ–°å¢å‚æ•°ï¼‰
                    use_enhanced = not args.no_enhanced_sampler
                    enable_dynamic = not args.no_dynamic_trials
                    enable_boundary = not args.no_boundary_search
                    
                    if not args.quiet:
                        print(f"\n[ä¼˜åŒ–] å¼€å§‹ä¼˜åŒ–...")
                        print(f"[ä¼˜åŒ–] åŸºç¡€è¯•éªŒæ¬¡æ•°: {args.trials}")
                        if use_enhanced:
                            print(f"[ä¼˜åŒ–] é‡‡æ ·ç­–ç•¥: æ­£æ€åˆ†å¸ƒ + è´å¶æ–¯ä¼˜åŒ–")
                        if enable_dynamic:
                            print(f"[ä¼˜åŒ–] åŠ¨æ€è¯•éªŒ: å¯ç”¨ï¼ˆå°†æ ¹æ®å‚æ•°é‡è‡ªåŠ¨è°ƒæ•´ï¼‰")
                        if enable_boundary:
                            print(f"[ä¼˜åŒ–] è¾¹ç•ŒäºŒæ¬¡æœç´¢: å¯ç”¨ï¼ˆæœ€å¤š{args.max_boundary_rounds}è½®ï¼‰\n")
                    
                    result = optimizer.optimize(
                        n_trials=args.trials,
                        use_enhanced_sampler=use_enhanced,
                        enable_dynamic_trials=enable_dynamic,
                        auto_expand_boundary=enable_boundary,
                        max_expansion_rounds=args.max_boundary_rounds
                    )
                    
                    # æ‰“å°å’Œä¿å­˜ç»“æœ
                    print_results(result, asset_output_dir, original_asset_name)
                    
                    # è®°å½•ç»“æœ
                    all_results.append({
                        'asset': original_asset_name,
                        'status': 'success',
                        'result': result
                    })
                    success_count += 1
                    
                except Exception as e:
                    print(f"\nâŒ ä¼˜åŒ– {original_asset_name} å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    all_results.append({
                        'asset': original_asset_name,
                        'status': 'failed',
                        'error': str(e)
                    })
                    fail_count += 1
                    continue
        
        # 4. æ‰“å°æ‰¹é‡ä¼˜åŒ–æ±‡æ€»
        print("\n" + "="*60)
        print("ğŸ“Š æ‰¹é‡ä¼˜åŒ–æ±‡æ€»")
        print("="*60)
        print(f"æ€»è®¡: {len(data_files)} ä¸ªæ ‡çš„")
        print(f"æˆåŠŸ: {success_count} ä¸ª")
        print(f"å¤±è´¥: {fail_count} ä¸ª")
        
        if success_count > 0:
            print("\nã€å„æ ‡çš„æœ€ä¼˜ç»“æœã€‘")
            print("-" * 60)
            print(f"{'æ ‡çš„':<15} {'å¤æ™®æ¯”ç‡':>12} {'å¹´åŒ–æ”¶ç›Š':>12} {'æœ€å¤§å›æ’¤':>12}")
            print("-" * 60)
            
            for item in all_results:
                if item['status'] == 'success':
                    metrics = item['result'].get('performance_metrics', {})
                    sharpe = metrics.get('sharpe_ratio', 0)
                    annual_ret = metrics.get('annual_return', 0)
                    max_dd = metrics.get('max_drawdown', 0)
                    print(f"{item['asset']:<15} {sharpe:>12.4f} {annual_ret:>11.2f}% {max_dd:>11.2f}%")
            
            print("-" * 60)
        
        # 5. ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_path = output_dir / "batch_optimization_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("æ‰¹é‡ç­–ç•¥ä¼˜åŒ–ç»“æœæ±‡æ€»\n")
            f.write("="*60 + "\n\n")
            f.write(f"ä¼˜åŒ–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç­–ç•¥æ–‡ä»¶: {args.strategy}\n")
            f.write(f"ä¼˜åŒ–ç›®æ ‡: {args.objective}\n")
            f.write(f"è¯•éªŒæ¬¡æ•°: {args.trials}\n")
            f.write(f"æ ‡çš„æ€»æ•°: {len(data_files)}\n")
            f.write(f"æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}\n\n")
            
            f.write("-"*60 + "\n")
            f.write(f"{'æ ‡çš„':<15} {'å¤æ™®æ¯”ç‡':>12} {'å¹´åŒ–æ”¶ç›Š':>12} {'æœ€å¤§å›æ’¤':>12}\n")
            f.write("-"*60 + "\n")
            
            for item in all_results:
                if item['status'] == 'success':
                    metrics = item['result'].get('performance_metrics', {})
                    sharpe = metrics.get('sharpe_ratio', 0)
                    annual_ret = metrics.get('annual_return', 0)
                    max_dd = metrics.get('max_drawdown', 0)
                    f.write(f"{item['asset']:<15} {sharpe:>12.4f} {annual_ret:>11.2f}% {max_dd:>11.2f}%\n")
                else:
                    f.write(f"{item['asset']:<15} {'å¤±è´¥':>12} {item.get('error', '')[:30]}\n")
            
            f.write("-"*60 + "\n")
        
        # ä¿å­˜JSONæ±‡æ€»
        json_summary_path = output_dir / "batch_optimization_summary.json"
        json_summary = {
            'optimization_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'strategy': args.strategy,
            'objective': args.objective,
            'trials': args.trials,
            'total_assets': len(data_files),
            'success_count': success_count,
            'fail_count': fail_count,
            'results': []
        }
        
        for item in all_results:
            if item['status'] == 'success':
                json_summary['results'].append({
                    'asset': item['asset'],
                    'status': 'success',
                    'best_parameters': item['result'].get('best_parameters', {}),
                    'performance_metrics': item['result'].get('performance_metrics', {})
                })
            else:
                json_summary['results'].append({
                    'asset': item['asset'],
                    'status': 'failed',
                    'error': item.get('error', '')
                })
        
        with open(json_summary_path, 'w', encoding='utf-8') as f:
            json.dump(json_summary, f, indent=2, ensure_ascii=False)
        
        print(f"\næ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_path}")
        print(f"JSONæ±‡æ€»: {json_summary_path}")
        
        print("\n" + "="*60)
        print("âœ… æ‰¹é‡ä¼˜åŒ–å®Œæˆï¼")
        print("="*60 + "\n")
        
        return 0 if fail_count == 0 else 1
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
        
    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
